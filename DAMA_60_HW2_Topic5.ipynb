{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYH6wsUSbrCKHSIhJIesxf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zl5CVFfMk4dI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import hashlib\n",
        "from nltk.corpus import gutenberg\n",
        "from typing import Set, List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The next instruction downloads the corpus\n",
        "# ATTENTION!!! Run this instruction once only for each Python installation,\n",
        "# then comment it!!\n",
        "\n",
        "#nltk.download(\"gutenberg\")"
      ],
      "metadata": {
        "id": "nbkbKFvmlapx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shingle_size = 3  # The character length of each shingle\n",
        "\n",
        "b = 4  # The number of bands for LSH\n",
        "r = 3  # The number of rows per band for LSH\n",
        "num_hash_functions = b * r\t# The number of the minhash functions\n",
        "\n",
        "# The size of the prefix of each document to be loaded\n",
        "DOCLENGTH = 3000"
      ],
      "metadata": {
        "id": "XxKp8GlLlms2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts and returns as a set all shingles of size shingle_size from document doc\n",
        "def extract_shingles(doc : str, shingle_size : int):\n",
        "    shingles = set()\t\t\t# initially shingles is an empty set\n",
        "    for i in range(len(doc)):\t\t# Iterate over entire document, one character at a time and\n",
        "        shingle = doc[i: i + shingle_size]\t\t#    extract shingle from current character\n",
        "        shingles.add(shingle)   # Since shingles is a set, if a shingle is already in it, it is not added again\n",
        "    return shingles"
      ],
      "metadata": {
        "id": "vt-wosAismmW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes and returns the Jaccard similarity\n",
        "# between two sets of shingles, s1 and s2\n",
        "def jaccard_similarity(s1: Set[str], s2: Set[str]):\n",
        "    intersection = s1.intersection(s2)\n",
        "    union = s1.union(s2)\n",
        "    return len(intersection) / len(union)"
      ],
      "metadata": {
        "id": "qXTx_fNduOhl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computes and returns a signature for a set of shingles,\n",
        "# with num_hash_functions hash functions\n",
        "def minhash_signature(shingles : Set[str], num_hash_functions : int):\n",
        "    signature = []\t\t# Initially signature is an empty list\n",
        "    for i in range(num_hash_functions):\n",
        "        min_hash = float('inf')\n",
        "        for shingle in shingles:\n",
        "                # The next instriction creates a hash value that depends on 'shingle',\n",
        "                # concatenated with the number of the hash function as a string.\n",
        "                # In this way we create easily multiple hash functions (one for each\n",
        "                # hash function id), not by changing the hash function\n",
        "                # (which is always the md5),  but by changing the strings to be hashed.\n",
        "                # hexdigest() method returns the hash value as a string (instead of\n",
        "                # a hash object) in hexadecimal format, whereas int(…, 16)\n",
        "                # converts this string to an integer.\n",
        "                # Note that the number of possible hash values returned by md5() is extra large,\n",
        "                # so we take the modulo of the returned value to a smaller value, e.g. 10000.\n",
        "\t\t\t\t        # However, the code works even without taking the modulo.\n",
        "                # Note finally that hashlib.md5() (as well as other hash functions from\n",
        "                # hashlib) takes as input byte strings only, thus the need for “encode()”.\n",
        "            hash_value = int(hashlib.md5((str(i) + shingle).encode()).hexdigest(), 16) % 10000\n",
        "            min_hash = min(min_hash, hash_value)\n",
        "        signature.append(min_hash)             # You have to add min_hash at the end of list signature\n",
        "    return signature"
      ],
      "metadata": {
        "id": "lRaFniKQvvl4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function distributes the signatures in buckets\n",
        "def lsh(signatures : List[List[int]], b : int, r : int):\n",
        "    # buckets are defined as a dictionary. The keys in the dictionary are the bucket numbers (integers)\n",
        "\t# In this implementation the set buckets is shared by all bands\n",
        "    buckets = {}\n",
        "    for doc_num, signature in enumerate(signatures):\n",
        "        # doc_num takes increasing integer values, starting from 0\n",
        "\t\t# signature takes the values in signatures\n",
        "        for band_num in range(b):\n",
        "\t\t\t# band is assigned a sublist of the signature, with r only numbers\n",
        "            band = tuple(signature[band_num * r:(band_num + 1) * r])\n",
        "            \t# hash() method returns a hash value for any python object.\n",
        "                # Compared to hashlib.md5() it has two differences:\n",
        "                # i) It gets as input any immutable object, instead of byte strings\n",
        "                # ii) It is randomized at the start of the python session,\n",
        "                # so identical objects during different python sessions get different\n",
        "                # hash values.\n",
        "                # hash() computes a hash value for the part of the signature in the band.\n",
        "                # Note that hash() function returns a huge number of possible hash\n",
        "                # values, thus for two bands to be hashed in the same bucket they\n",
        "                # have to be identical, except if we take the modulo to some smaller value.\n",
        "            hash_value = hash(band) % 10000\n",
        "            if hash_value in buckets: \t# there is already another document\n",
        "                                        # in the same bucket (possibly by another band)\n",
        "                buckets[hash_value].add(doc_num)\n",
        "            else:\n",
        "                buckets[hash_value] = {doc_num}   \t# this is the first\n",
        "                                                    # document in the particular bucket\n",
        "    return buckets"
      ],
      "metadata": {
        "id": "lf9bu5g6zyGW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns all pairs of documents that co-appear in some bucket\n",
        "def find_candidate_pairs(buckets : Dict[int, Set[int]]):\n",
        "    candidate_pairs = set()\t\t# initially an empty set\n",
        "    for bucket in buckets.values():\n",
        "        if len(bucket) > 1:\n",
        "            for doc1 in bucket:\n",
        "                for doc2 in bucket:\n",
        "                    if doc1 < doc2:\n",
        "                        candidate_pairs.add((doc1, doc2))\n",
        "    return candidate_pairs"
      ],
      "metadata": {
        "id": "egjZndZV5xgI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load three document from each one of three authors (nine documents totally)\n",
        "docs = [\n",
        "    # Reading the first DOCLENGTH chars of each document\n",
        "    gutenberg.raw(\"austen-emma.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"austen-persuasion.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"austen-sense.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"chesterton-ball.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"chesterton-brown.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"chesterton-thursday.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"shakespeare-caesar.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"shakespeare-hamlet.txt\")[:DOCLENGTH],\n",
        "    gutenberg.raw(\"shakespeare-macbeth.txt\")[:DOCLENGTH] ]"
      ],
      "metadata": {
        "id": "sd52ULRk591s"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the set of shingles for each document\n",
        "shingle_sets = [extract_shingles(doc, shingle_size) for doc in docs]"
      ],
      "metadata": {
        "id": "kNGg17rv6D4l"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Jaccard similarity between all pairs of documents\n",
        "print(\"Jaccard Similarities:\")\n",
        "for i in range(len(docs)):\n",
        "    for j in range(i + 1, len(docs)):\n",
        "        similarity = jaccard_similarity(shingle_sets[i], shingle_sets[j])\n",
        "        print(f\"Document {i} and Document {j}: {similarity:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqnU0iT06G-V",
        "outputId": "b6855096-e67a-4cf8-9b2b-1e7d15570354"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarities:\n",
            "Document 0 and Document 1: 0.323\n",
            "Document 0 and Document 2: 0.386\n",
            "Document 0 and Document 3: 0.323\n",
            "Document 0 and Document 4: 0.308\n",
            "Document 0 and Document 5: 0.295\n",
            "Document 0 and Document 6: 0.230\n",
            "Document 0 and Document 7: 0.236\n",
            "Document 0 and Document 8: 0.242\n",
            "Document 1 and Document 2: 0.332\n",
            "Document 1 and Document 3: 0.282\n",
            "Document 1 and Document 4: 0.284\n",
            "Document 1 and Document 5: 0.260\n",
            "Document 1 and Document 6: 0.227\n",
            "Document 1 and Document 7: 0.226\n",
            "Document 1 and Document 8: 0.224\n",
            "Document 2 and Document 3: 0.321\n",
            "Document 2 and Document 4: 0.311\n",
            "Document 2 and Document 5: 0.291\n",
            "Document 2 and Document 6: 0.238\n",
            "Document 2 and Document 7: 0.238\n",
            "Document 2 and Document 8: 0.241\n",
            "Document 3 and Document 4: 0.311\n",
            "Document 3 and Document 5: 0.282\n",
            "Document 3 and Document 6: 0.236\n",
            "Document 3 and Document 7: 0.241\n",
            "Document 3 and Document 8: 0.233\n",
            "Document 4 and Document 5: 0.293\n",
            "Document 4 and Document 6: 0.240\n",
            "Document 4 and Document 7: 0.230\n",
            "Document 4 and Document 8: 0.234\n",
            "Document 5 and Document 6: 0.232\n",
            "Document 5 and Document 7: 0.244\n",
            "Document 5 and Document 8: 0.253\n",
            "Document 6 and Document 7: 0.315\n",
            "Document 6 and Document 8: 0.298\n",
            "Document 7 and Document 8: 0.295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute minhash signatures for each document\n",
        "signatures = [minhash_signature(shingles, num_hash_functions) for shingles in shingle_sets]"
      ],
      "metadata": {
        "id": "SpuGGtgZ6N2f"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribute signatures to buckets using lsh\n",
        "buckets = lsh(signatures, b, r)"
      ],
      "metadata": {
        "id": "zoODw4d36Q9W"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find candidate pairs from buckets\n",
        "candidate_pairs = find_candidate_pairs(buckets)\n",
        "print(\"\\nCandidate Pairs (from LSH):\")\n",
        "for pair in candidate_pairs:\n",
        "    print(f\"Document {pair[0]} and Document {pair[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs6rQ2Pk6TgH",
        "outputId": "c3214349-fb27-4e26-af40-b7ce9499e383"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Candidate Pairs (from LSH):\n",
            "Document 2 and Document 5\n",
            "Document 3 and Document 4\n",
            "Document 7 and Document 8\n",
            "Document 0 and Document 6\n"
          ]
        }
      ]
    }
  ]
}